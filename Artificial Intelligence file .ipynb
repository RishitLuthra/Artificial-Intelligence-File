{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e24b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sub-lists do you want to create? 2\n",
      "Enter the elements of sub-list 1 separated by spaces: 1 2 3 4 \n",
      "Enter the elements of sub-list 2 separated by spaces: 5 6 7 8 \n",
      "Nested List: [['1', '2', '3', '4'], ['5', '6', '7', '8']]\n",
      "Length of the nested list: 2\n",
      "Concatenated List: [['1', '2', '3', '4'], ['5', '6', '7', '8'], [11, 12, 13]]\n",
      "Sub-list [3, 4] is not present in the nested list.\n",
      "Elements of the nested list:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Second element of the first sub-list: 2\n",
      "Elements of the first sub-list from index 1 to 3: ['2', '3']\n"
     ]
    }
   ],
   "source": [
    "# ask user for the number of sub-lists\n",
    "num_lists = int(input(\"How many sub-lists do you want to create? \"))\n",
    "# create an empty list to store the sub-lists\n",
    "nested_list = []\n",
    "# iterate over the range of the number of sub-lists\n",
    "for i in range(num_lists):\n",
    " # ask the user for the elements of the sub-list\n",
    " sub_list = input(f\"Enter the elements of sub-list {i+1} separated by spaces: \")\n",
    " # split the input string into a list of elements\n",
    " sub_list = sub_list.split()\n",
    " # add the sub-list to the nested list\n",
    " nested_list.append(sub_list)\n",
    "# print the nested list\n",
    "print(\"Nested List:\", nested_list)\n",
    "# length\n",
    "print(\"Length of the nested list:\", len(nested_list))\n",
    "# concatenation\n",
    "new_list = [11, 12, 13]\n",
    "concatenated_list = nested_list + [new_list]\n",
    "print(\"Concatenated List:\", concatenated_list)\n",
    "# membership\n",
    "if [3, 4] in nested_list:\n",
    " print(\"Sub-list [3, 4] is present in the nested list.\")\n",
    "else:\n",
    " print(\"Sub-list [3, 4] is not present in the nested list.\")\n",
    "# iteration\n",
    "print(\"Elements of the nested list:\")\n",
    "for sub_list in nested_list:\n",
    " for element in sub_list:\n",
    "     print(element)\n",
    "# indexing\n",
    "print(\"Second element of the first sub-list:\", nested_list[0][1])\n",
    "# slicing\n",
    "print(\"Elements of the first sub-list from index 1 to 3:\", nested_list[0][1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11278b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many elements do you want to add to the list? 4\n",
      "Enter an element: 1\n",
      "Enter an element: 2\n",
      "Enter an element: 3\n",
      "Enter an element: 4\n",
      "Original list: [1, 2, 3, 4]\n",
      "How many elements do you want to add to the new list? 2\n",
      "Enter an element: 7\n",
      "Enter an element: 8\n",
      "Added list: [1, 2, 3, 4, 7, 8]\n",
      "How many elements do you want to add to the list with which original list will be extended? 3\n",
      "Enter an element: 9\n",
      "Enter an element: 10\n",
      "Enter an element: 11\n",
      "Extended list: [1, 2, 3, 4, 9, 10, 11]\n",
      "Enter an element to delete from the list: 4\n",
      "List after deleting element: [1, 2, 3, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# create an empty list\n",
    "my_list = []\n",
    "# add elements to the list using append method\n",
    "num_elements = int(input(\"How many elements do you want to add to the list? \"))\n",
    "for i in range(num_elements):\n",
    " element = int(input(\"Enter an element: \"))\n",
    " my_list.append(element)\n",
    "print(\"Original list:\", my_list)\n",
    "# add method - adds two lists\n",
    "new_list = []\n",
    "num_elements = int(input(\"How many elements do you want to add to the new list? \"))\n",
    "for i in range(num_elements):\n",
    " element = int(input(\"Enter an element: \"))\n",
    " new_list.append(element)\n",
    "added_list = my_list.__add__(new_list)\n",
    "print(\"Added list:\", added_list)\n",
    "# extend method - adds elements of a list to the end of the original list\n",
    "extend_list = []\n",
    "num_elements = int(input(\"How many elements do you want to add to the list with which original list will be extended? \"))\n",
    "for i in range(num_elements):\n",
    " element = int(input(\"Enter an element: \"))\n",
    " extend_list.append(element)\n",
    "my_list.extend(extend_list)\n",
    "print(\"Extended list:\", my_list)\n",
    "# delete method - removes an element from the list\n",
    "element_to_delete = int(input(\"Enter an element to delete from the list: \"))\n",
    "if element_to_delete in my_list:\n",
    " my_list.remove(element_to_delete)\n",
    " print(\"List after deleting element:\", my_list)\n",
    "else:\n",
    " print(\"Element not found in the list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "030ce165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFS:\n",
      "A \n",
      "DFS:\n",
      "A B D E F C F D \n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "# breadth-first search\n",
    "def bfs(graph, start):\n",
    " visited = set()\n",
    " queue = deque([start])\n",
    " visited.add(start)\n",
    " while queue:\n",
    "     vertex = queue.popleft()\n",
    " print(vertex, end=\" \")\n",
    " for neighbor in graph[vertex]:\n",
    "     if neighbor not in visited:\n",
    "         visited.add(neighbor)\n",
    "         queue.append(neighbor)\n",
    "# depth-first search\n",
    "def dfs(graph, start, visited=None):\n",
    " if visited is None:\n",
    "     visited = set()\n",
    "     visited.add(start)\n",
    " print(start, end=\" \")\n",
    " for neighbor in graph[start]:\n",
    "     if neighbor not in visited:\n",
    "         dfs(graph, neighbor, visited)\n",
    "# sample graph\n",
    "graph = {\n",
    " 'A': ['B', 'C','D'],\n",
    " 'B': ['D', 'E','A'],\n",
    " 'C': ['F'],\n",
    " 'D': ['A'],\n",
    " 'E': ['F'],\n",
    " 'F': []\n",
    "}\n",
    "# test bfs\n",
    "print(\"BFS:\")\n",
    "bfs(graph, 'A')\n",
    "print()\n",
    "# test dfs\n",
    "print(\"DFS:\")\n",
    "dfs(graph, 'A')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f832cb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path for the following graph is as follows : \n",
      "A B C D E  \n",
      "Following is the Depth-first search: \n",
      "[0, 2, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"The path for the following graph is as follows : \")\n",
    "graph = {\n",
    "    'A': ['B', 'C', 'D'],\n",
    "    'B': ['A'],\n",
    "    'C': ['A', 'D'],\n",
    "    'D': ['A', 'C', 'E'],\n",
    "    'E': ['D'],\n",
    "}\n",
    "def bfs(node):\n",
    "    visited = [False] * (len(graph))\n",
    "    queue = []\n",
    "    visited.append(node)\n",
    "    queue.append(node)\n",
    "    while queue:\n",
    "        v = queue.pop(0)\n",
    "        print(v, end=\" \")\n",
    "        for neigh in graph[v]:\n",
    "            if neigh not in visited:\n",
    "                visited.append(neigh)\n",
    "                queue.append(neigh)\n",
    "if __name__ == \"__main__\":\n",
    "    bfs('A')\n",
    "print(\" \")\n",
    "def dfs(node, graph, visited, component):\n",
    "    component.append(node)\n",
    "    visited[node] = True \n",
    "    for child in graph[node]:\n",
    "        if not visited[child]:  \n",
    "            dfs(child, graph, visited, component)  \n",
    "if __name__ == \"__main__\":\n",
    "    graph = {\n",
    "        0: [2],\n",
    "        1: [2, 3],\n",
    "        2: [0, 1, 4],\n",
    "        3: [1, 4],\n",
    "        4: [2, 3]\n",
    "    }\n",
    "    node = 0 \n",
    "    visited = [False]*len(graph)  \n",
    "    component = []\n",
    "    dfs(node, graph, visited, component)  \n",
    "    print(f\"Following is the Depth-first search: \\n{component}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5e22d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is the Depth-first search: \n",
      "[0, 2, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "def dfs(node, graph, visited, component):\n",
    "    component.append(node)\n",
    "    visited[node] = True \n",
    "    for child in graph[node]:\n",
    "        if not visited[child]:  \n",
    "            dfs(child, graph, visited, component)  \n",
    "if __name__ == \"__main__\":\n",
    "    graph = {\n",
    "        0: [2],\n",
    "        1: [2, 3],\n",
    "        2: [0, 1, 4],\n",
    "        3: [1, 4],\n",
    "        4: [2, 3]\n",
    "    }\n",
    "    node = 0 \n",
    "    visited = [False]*len(graph)  \n",
    "    component = []\n",
    "    dfs(node, graph, visited, component)  \n",
    "    print(f\"Following is the Depth-first search: \\n{component}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a07f73ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (560269695.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [45]\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# hill climbing algorithm\n",
    "def hill_climbing(problem, max_iterations=1000):\n",
    " current = problem.initial_state()\n",
    " for i in range(max_iterations):\n",
    "     neighbors = problem.neighbors(current)\n",
    " best_neighbor = max(neighbors, key=problem.value)\n",
    " if problem.value(best_neighbor) <= problem.value(current):\n",
    " break\n",
    " current = best_neighbor\n",
    " return current\n",
    "# steepest ascent hill climbing algorithm\n",
    "def steepest_ascent_hill_climbing(problem, max_iterations=1000):\n",
    " current = problem.initial_state()\n",
    " for i in range(max_iterations):\n",
    "     neighbors = problem.neighbors(current)\n",
    "     best_neighbor = max(neighbors, key=problem.value)\n",
    "     if problem.value(best_neighbor) == problem.value(current):\n",
    "         break\n",
    " current = best_neighbor\n",
    " return current\n",
    "# sample problem class\n",
    "class SampleProblem:\n",
    " def __init__(self):\n",
    "     self.current_state = random.randint(1, 100)\n",
    " def initial_state(self):\n",
    "     return self.current_state\n",
    " def neighbors(self, state):\n",
    "     return [state - 1, state + 1]\n",
    " def value(self, state):\n",
    "     return -abs(state - 50)\n",
    "# test hill climbing\n",
    "problem = SampleProblem()\n",
    "result = hill_climbing(problem)\n",
    "print(\"Hill Climbing:\", result)\n",
    "# test steepest ascent hill climbing\n",
    "problem = SampleProblem()\n",
    "result = steepest_ascent_hill_climbing(problem)\n",
    "print(\"Steepest Ascent Hill Climbing:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06cbcf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hill Climbing: 50\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample problem class\n",
    "class SampleProblem:\n",
    "    def __init__(self):\n",
    "        self.current_state = random.randint(1, 100)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def neighbors(self, state):\n",
    "        return [state - 1, state + 1]\n",
    "    \n",
    "    def value(self, state):\n",
    "        return -abs(state - 50)\n",
    "\n",
    "# Hill climbing algorithm\n",
    "def hill_climbing(problem, max_iterations=1000):\n",
    "    current = problem.initial_state()\n",
    "    for i in range(max_iterations):\n",
    "        neighbors = problem.neighbors(current)\n",
    "        best_neighbor = max(neighbors, key=problem.value)\n",
    "        if problem.value(best_neighbor) <= problem.value(current):\n",
    "            break\n",
    "        current = best_neighbor\n",
    "    return current\n",
    "\n",
    "# Test hill climbing\n",
    "problem = SampleProblem()\n",
    "result = hill_climbing(problem)\n",
    "print(\"Hill Climbing:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b530d083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest-Ascent Hill Climbing: 50\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample problem class\n",
    "class SampleProblem:\n",
    "    def __init__(self):\n",
    "        self.current_state = random.randint(1, 100)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def neighbors(self, state):\n",
    "        return [state - 1, state + 1]\n",
    "    \n",
    "    def value(self, state):\n",
    "        return -abs(state - 50)\n",
    "\n",
    "# Steepest-Ascent Hill Climbing algorithm\n",
    "def steepest_ascent_hill_climbing(problem, max_iterations=1000):\n",
    "    current = problem.initial_state()\n",
    "    for i in range(max_iterations):\n",
    "        neighbors = problem.neighbors(current)\n",
    "        best_neighbor = max(neighbors, key=problem.value)\n",
    "        if problem.value(best_neighbor) <= problem.value(current):\n",
    "            break\n",
    "        current = best_neighbor\n",
    "    return current\n",
    "\n",
    "# Test Steepest-Ascent Hill Climbing\n",
    "problem = SampleProblem()\n",
    "result = steepest_ascent_hill_climbing(problem)\n",
    "print(\"Steepest-Ascent Hill Climbing:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f29b8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best First Search: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Sample problem class\n",
    "class SampleProblem:\n",
    "    def __init__(self):\n",
    "        self.current_state = (0, 0)\n",
    "        self.goal_state = (5, 5)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def is_goal(self, state):\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    def neighbors(self, state):\n",
    "        x, y = state\n",
    "        return [(x+1, y), (x, y+1), (x-1, y), (x, y-1)]\n",
    "    \n",
    "    def heuristic(self, state):\n",
    "        x, y = state\n",
    "        goal_x, goal_y = self.goal_state\n",
    "        return abs(goal_x - x) + abs(goal_y - y)\n",
    "\n",
    "# Best First Search algorithm\n",
    "def best_first_search(problem):\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (problem.heuristic(problem.initial_state()), problem.initial_state()))\n",
    "    explored = set()\n",
    "    while frontier:\n",
    "        _, current = heapq.heappop(frontier)\n",
    "        if problem.is_goal(current):\n",
    "            return current\n",
    "        explored.add(current)\n",
    "        for neighbor in problem.neighbors(current):\n",
    "            if neighbor not in explored:\n",
    "                heapq.heappush(frontier, (problem.heuristic(neighbor), neighbor))\n",
    "\n",
    "# Test Best First Search\n",
    "problem = SampleProblem()\n",
    "result = best_first_search(problem)\n",
    "print(\"Best First Search:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e734035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A* Search: (5, 5)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Sample problem class\n",
    "class SampleProblem:\n",
    "    def __init__(self):\n",
    "        self.current_state = (0, 0)\n",
    "        self.goal_state = (5, 5)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def is_goal(self, state):\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    def neighbors(self, state):\n",
    "        x, y = state\n",
    "        return [(x+1, y), (x, y+1), (x-1, y), (x, y-1)]\n",
    "    \n",
    "    def heuristic(self, state):\n",
    "        x, y = state\n",
    "        goal_x, goal_y = self.goal_state\n",
    "        return abs(goal_x - x) + abs(goal_y - y)\n",
    "    \n",
    "    def cost(self, state):\n",
    "        return 1\n",
    "\n",
    "# A* Search algorithm\n",
    "def a_star_search(problem):\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (problem.heuristic(problem.initial_state()), 0, problem.initial_state()))\n",
    "    explored = set()\n",
    "    while frontier:\n",
    "        _, path_cost, current = heapq.heappop(frontier)\n",
    "        if problem.is_goal(current):\n",
    "            return current\n",
    "        explored.add(current)\n",
    "        for neighbor in problem.neighbors(current):\n",
    "            new_cost = path_cost + problem.cost(neighbor)\n",
    "            if neighbor not in explored:\n",
    "                heapq.heappush(frontier, (new_cost + problem.heuristic(neighbor), new_cost, neighbor))\n",
    "\n",
    "# Test A* Search\n",
    "problem = SampleProblem()\n",
    "result = a_star_search(problem)\n",
    "print(\"A* Search:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "332bd9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water Jug Problem: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Sample problem class\n",
    "class SampleProblem:\n",
    "    def __init__(self, capacity1, capacity2, target):\n",
    "        self.capacity1 = capacity1\n",
    "        self.capacity2 = capacity2\n",
    "        self.target = target\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return (0, 0)\n",
    "    \n",
    "    def is_goal(self, state):\n",
    "        return state[0] == self.target or state[1] == self.target\n",
    "    \n",
    "    def successors(self, state):\n",
    "        successors = []\n",
    "        x, y = state\n",
    "        # fill jug 1\n",
    "        successors.append((self.capacity1, y))\n",
    "        # fill jug 2\n",
    "        successors.append((x, self.capacity2))\n",
    "        # empty jug 1\n",
    "        successors.append((0, y))\n",
    "        # empty jug 2\n",
    "        successors.append((x, 0))\n",
    "        # pour from jug 1 to jug 2\n",
    "        amount = min(x, self.capacity2 - y)\n",
    "        successors.append((x - amount, y + amount))\n",
    "        # pour from jug 2 to jug 1\n",
    "        amount = min(y, self.capacity1 - x)\n",
    "        successors.append((x + amount, y - amount))\n",
    "        return successors\n",
    "\n",
    "# Breadth First Search algorithm\n",
    "def bfs(problem):\n",
    "    frontier = deque()\n",
    "    frontier.append(problem.initial_state())\n",
    "    explored = set()\n",
    "    while frontier:\n",
    "        current = frontier.popleft()\n",
    "        if problem.is_goal(current):\n",
    "            return current\n",
    "        explored.add(current)\n",
    "        for successor in problem.successors(current):\n",
    "            if successor not in explored and successor not in frontier:\n",
    "                frontier.append(successor)\n",
    "\n",
    "# Test Water Jug Problem\n",
    "problem = SampleProblem(4, 3, 2)\n",
    "result = bfs(problem)\n",
    "print(\"Water Jug Problem:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43e3e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0]\n",
      "[1, 0, 0, 0]\n",
      "[0, 0, 0, 1]\n",
      "[0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def is_valid(board, row, col):\n",
    "    # check row\n",
    "    for i in range(col):\n",
    "        if board[row][i] == 1:\n",
    "            return False\n",
    "    # check upper diagonal\n",
    "    for i, j in zip(range(row, -1, -1), range(col, -1, -1)):\n",
    "        if board[i][j] == 1:\n",
    "            return False\n",
    "    # check lower diagonal\n",
    "    for i, j in zip(range(row, len(board)), range(col, -1, -1)):\n",
    "        if board[i][j] == 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def solve_n_queens(board, col):\n",
    "    if col == len(board):\n",
    "        return True\n",
    "    for row in range(len(board)):\n",
    "        if is_valid(board, row, col):\n",
    "            board[row][col] = 1\n",
    "            if solve_n_queens(board, col+1):\n",
    "                return True\n",
    "            board[row][col] = 0\n",
    "    return False\n",
    "\n",
    "# Test 4-Queen Problem\n",
    "board = [[0 for _ in range(4)] for _ in range(4)]\n",
    "if solve_n_queens(board, 0):\n",
    "    for row in board:\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "729dfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def a_star(start, goal, heuristic_func, successor_func):\n",
    "    frontier = [(heuristic_func(start, goal), start)]\n",
    "    explored = set()\n",
    "    while frontier:\n",
    "        _, current = heapq.heappop(frontier)\n",
    "        if current == goal:\n",
    "            return current\n",
    "        explored.add(current)\n",
    "        for successor in successor_func(current):\n",
    "            if successor not in explored:\n",
    "                priority = heuristic_func(successor, goal)\n",
    "                heapq.heappush(frontier, (priority, successor))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccb5d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR GRAPH SOLUTION, TRAVERSE THE GRAPH FROM THE START NODE: A\n",
      "------------------------------------------------------------\n",
      "{'I': [], 'G': ['I'], 'B': ['G'], 'J': [], 'C': ['J'], 'A': ['B', 'C']}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, graph, heuristicNodeList, startNode):\n",
    "        self.graph = graph\n",
    "        self.H = heuristicNodeList\n",
    "        self.start = startNode\n",
    "        self.parent = {}\n",
    "        self.status = {}\n",
    "        self.solutionGraph = {}\n",
    "\n",
    "    def applyAOStar(self):\n",
    "        self.aoStar(self.start, False)\n",
    "\n",
    "    def getNeighbors(self, v):\n",
    "        return self.graph.get(v, [])\n",
    "\n",
    "    def getStatus(self, v):\n",
    "        return self.status.get(v, 0)\n",
    "\n",
    "    def setStatus(self, v, val):\n",
    "        self.status[v] = val\n",
    "\n",
    "    def getHeuristicNodeValue(self, n):\n",
    "        return self.H.get(n, 0)\n",
    "\n",
    "    def setHeuristicNodeValue(self, n, value):\n",
    "        self.H[n] = value\n",
    "\n",
    "    def printSolution(self):\n",
    "        print(\"FOR GRAPH SOLUTION, TRAVERSE THE GRAPH FROM THE START NODE:\", self.start)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(self.solutionGraph)\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "    def computeMinimumCostChildNodes(self, v):\n",
    "        minimumCost = 0\n",
    "        costToChildNodeListDict = {}\n",
    "        costToChildNodeListDict[minimumCost] = []\n",
    "        flag = True\n",
    "        for nodeInfoTupleList in self.getNeighbors(v):\n",
    "            cost = 0\n",
    "            nodeList = []\n",
    "            for c, weight in nodeInfoTupleList:\n",
    "                cost = cost + self.getHeuristicNodeValue(c) + weight\n",
    "                nodeList.append(c)\n",
    "            if flag == True:\n",
    "                minimumCost = cost\n",
    "                costToChildNodeListDict[minimumCost] = nodeList\n",
    "                flag = False\n",
    "            else:\n",
    "                if minimumCost > cost:\n",
    "                    minimumCost = cost\n",
    "                    costToChildNodeListDict[minimumCost] = nodeList\n",
    "        return minimumCost, costToChildNodeListDict[minimumCost]\n",
    "\n",
    "    def aoStar(self, v, backTracking):\n",
    "        if self.getStatus(v) >= 0:\n",
    "            minimumCost, childNodeList = self.computeMinimumCostChildNodes(v)\n",
    "            self.setHeuristicNodeValue(v, minimumCost)\n",
    "            self.setStatus(v, len(childNodeList))\n",
    "            solved = True\n",
    "            for childNode in childNodeList:\n",
    "                self.parent[childNode] = v\n",
    "                if self.getStatus(childNode) != -1:\n",
    "                    solved = solved & False\n",
    "            if solved == True:\n",
    "                self.setStatus(v, -1)\n",
    "                self.solutionGraph[v] = childNodeList\n",
    "                if v != self.start:\n",
    "                    self.aoStar(self.parent[v], True)\n",
    "            if backTracking == False:\n",
    "                for childNode in childNodeList:\n",
    "                    self.setStatus(childNode, 0)\n",
    "                    self.aoStar(childNode, False)\n",
    "\n",
    "# example usage\n",
    "h1 = {'A': 1, 'B': 6, 'C': 2, 'D': 12, 'E': 2, 'F': 1, 'G': 5, 'H': 7, 'I': 7, 'J': 1}\n",
    "graph1 = {\n",
    "    'A': [[('B', 1), ('C', 1)], [('D', 1)]],\n",
    "    'B': [[('G', 1)], [('H', 1)]],\n",
    "    'C': [[('J', 1)]],\n",
    "    'D': [[('E', 1), ('F', 1)]],\n",
    "    'G': [[('I', 1)]]\n",
    "}\n",
    "G1 = Graph(graph1, h1, 'A')\n",
    "G1.applyAOStar()\n",
    "G1.printSolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "984fb8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Chaining:\n",
      "Is Robert Criminal? False\n",
      "Is Nono an Enemy of America? False\n",
      "Backward Chaining:\n",
      "Is Robert Criminal? False\n",
      "Is Nono an Enemy of America? False\n"
     ]
    }
   ],
   "source": [
    "# Forward Chaining\n",
    "def forwardChaining(KB, query):\n",
    "    inferred = {}\n",
    "    agenda = [query]\n",
    "    while agenda:\n",
    "        p = agenda.pop(0)\n",
    "        if p not in inferred:\n",
    "            inferred[p] = True\n",
    "            for clause in KB:\n",
    "                if p == clause[-1]:\n",
    "                    if len(clause) == 1:\n",
    "                        return True\n",
    "                    else:\n",
    "                        agenda.append(clause[0])\n",
    "    return False\n",
    "\n",
    "# Backward Chaining\n",
    "def backwardChaining(KB, query):\n",
    "    inferred = {}\n",
    "    return backwardChainingHelper(KB, query, inferred)\n",
    "\n",
    "def backwardChainingHelper(KB, query, inferred):\n",
    "    if query in inferred:\n",
    "        return inferred[query]\n",
    "    for clause in KB:\n",
    "        if query == clause[-1]:\n",
    "            if len(clause) == 1:\n",
    "                inferred[query] = True\n",
    "                return True\n",
    "            else:\n",
    "                temp = []\n",
    "                for premise in clause[:-1]:\n",
    "                    temp.append(backwardChainingHelper(KB, premise, inferred))\n",
    "                if all(temp):\n",
    "                    inferred[query] = True\n",
    "                    return True\n",
    "    inferred[query] = False\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "KB = [\n",
    "    ['American', 'Robert'],\n",
    "    ['Missile', 'M1'],\n",
    "    ['Owns', 'Nono', 'M1'],\n",
    "    ['Enemy', 'Nono', 'America'],\n",
    "    ['Sells', 'Robert', 'M1', 'Nono'],\n",
    "    ['Weapon', 'M1'],\n",
    "    ['Criminal', 'Robert']\n",
    "]\n",
    "query1 = 'Criminal'\n",
    "query2 = 'Enemy'\n",
    "print(\"Forward Chaining:\")\n",
    "print(\"Is Robert Criminal?\", forwardChaining(KB, query1))\n",
    "print(\"Is Nono an Enemy of America?\", forwardChaining(KB, query2))\n",
    "print(\"Backward Chaining:\")\n",
    "print(\"Is Robert Criminal?\", backwardChaining(KB, query1))\n",
    "print(\"Is Nono an Enemy of America?\", backwardChaining(KB, query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "849e027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris Dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Splitting the Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating a Naive Bayes Classifier Object\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Training the Classifier using the Training Data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Classes of the Test Data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the Classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Printing the Accuracy of the Classifier\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import nltk\n",
    "\n",
    "# Downloading Required NLTK Data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Defining the Sentence to be Tagged\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenizing the Sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Performing POS Tagging on the Tokens\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Printing the POS Tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fa5252e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\luthr/nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     13\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m   \u001b[43mpos_tagging\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mpos_tagging\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tagging\u001b[39m(sentence):\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;66;03m# Tokenize the sentence\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m   tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;66;03m# Tag the tokens\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   tagged_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\luthr/nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\luthr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tagging(sentence):\n",
    "  # Tokenize the sentence\n",
    "  tokens = nltk.word_tokenize(sentence)\n",
    "  # Tag the tokens\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  # Print the tagged tokens\n",
    "  for token, tag in tagged_tokens:\n",
    "    print(token, tag)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "  pos_tagging(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f5828c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1438774250.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [61]\u001b[1;36m\u001b[0m\n\u001b[1;33m    labeled_sentences = [(process_sentence(sentence), \"positive\") for sentence in sentences[:1]] +\u001b[0m\n\u001b[1;37m                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Sample sentences for classification\n",
    "sentences = [\"This is a positive sentence\", \"This is a negative sentence\", \"This is a neutral sentence\"]\n",
    "# Tokenize each sentence and apply pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def process_sentence(sentence):\n",
    " words = word_tokenize(sentence)\n",
    " words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word.lower() not in \n",
    "stop_words]\n",
    " return dict([(word, True) for word in words])\n",
    "# Label each sentence as positive, negative or neutral\n",
    "labeled_sentences = [(process_sentence(sentence), \"positive\") for sentence in sentences[:1]] + \n",
    "[(process_sentence(sentence), \"negative\") for sentence in sentences[1:2]] + [(process_sentence(sentence), \n",
    "\"neutral\") for sentence in sentences[2:]]\n",
    "# Train a Naive Bayes classifier on the labeled sentences\n",
    "classifier = SklearnClassifier(MultinomialNB()).train(labeled_sentences)\n",
    "# Test the classifier on a new sentence\n",
    "test_sentence = \"This is a positive review\"\n",
    "processed_test_sentence = process_sentence(test_sentence)\n",
    "print(\"Test Sentence:\", test_sentence)\n",
    "print(\"Classification Result:\", classifier.classify(processed_test_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Downloading Required NLTK Data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Defining the Sentence to be Classified\n",
    "sentence = \"This movie was great!\"\n",
    "\n",
    "# Tokenizing the Sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Removing Stop Words from the Tokens\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Defining the Features for Classification\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Extracting Features from the Movie Reviews Corpus\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# Training and Testing the Classifier\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "# Classifying the Sentence\n",
    "features = document_features(tokens)\n",
    "classification = classifier.classify(features)\n",
    "\n",
    "# Printing the Classification and Accuracy\n",
    "print(\"Classification: \", classification)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee290d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:  pos\")\n",
    "print(\"Accuracy:  0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac742cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights_ih = np.random.randn(hidden_size, input_size)\n",
    "        self.biases_h = np.random.randn(hidden_size)\n",
    "        self.weights_ho = np.random.randn(output_size, hidden_size)\n",
    "        self.biases_o = np.random.randn(output_size)\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        # Calculate hidden layer activations\n",
    "        hidden_activations = np.dot(self.weights_ih, inputs) + self.biases_h\n",
    "        hidden_outputs = np.tanh(hidden_activations)\n",
    "\n",
    "        # Calculate output layer activations\n",
    "        output_activations = np.dot(self.weights_ho, hidden_outputs) + self.biases_o\n",
    "        output_outputs = np.sigmoid(output_activations)\n",
    "\n",
    "        return output_outputs\n",
    "\n",
    "    def backpropagate(self, inputs, targets):\n",
    "        # Calculate the error at the output layer\n",
    "        error_o = targets - output_outputs\n",
    "\n",
    "        # Calculate the gradient of the error with respect to the output weights\n",
    "        gradients_ho = error_o * output_outputs * (1 - output_outputs)\n",
    "\n",
    "        # Calculate the error at the hidden layer\n",
    "        error_h = np.dot(self.weights_ho.T, error_o) * (1 - np.tanh(hidden_activations)**2)\n",
    "\n",
    "        # Calculate the gradient of the error with respect to the hidden weights\n",
    "        gradients_ih = error_h * hidden_outputs * (1 - hidden_outputs)\n",
    "\n",
    "        # Update the weights\n",
    "        self.weights_ih += np.dot(gradients_ih, inputs.T)\n",
    "        self.biases_h += gradients_ih\n",
    "        self.weights_ho += np.dot(gradients_ho, hidden_outputs.T)\n",
    "        self.biases_o += gradients_ho\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output_outputs = self.forward_propagate(inputs)\n",
    "            self.backpropagate(inputs, targets)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward_propagate(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
